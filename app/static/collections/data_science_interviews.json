[{"front": "Logistic function", "back": "\\[\\sigma(t) = \\frac{1}{1+e^{-t}}\\]<div><br></div><div><img src=\"/static/uploads/data_science_interviews/800px-Logistic-curve.svg.png\"><br></div><div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Logistic_function\">Ref</a> "}, {"front": "Central Limit Theorem", "back": "<div>Sampling the mean of any random variable gives a normal distribution with the same mean and stddev \\(\\frac{\\sigma} {\\sqrt N}\\).</div><div><br></div><div>(for finite mean, stddev \\(\\sigma\\), and \\(N\\) samples)</div><div><ul> </ul></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Central_limit_theorem\">Ref</a> "}, {"front": "What are the assumptions behind linear regression?", "back": "1. Relationship is actually linear (y is linear combo of x's)<div>2. No error in x values<br><div><div>3. Errors are independent and</div><div>4. have constant variance<br></div><div><br></div><div>(5. Non-collinear samples</div><div>is required for least-squares methods, but not others)</div></div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Linear_regression#Assumptions\">Ref</a> "}, {"front": "In linear regression, what does the p-value of a coefficient tell you?", "back": "The chance the variable doesn't matter.<div><br></div><div>(Technically: given that the coefficent&nbsp;<i>is</i>&nbsp;zero, the probablity you erroneously find it non-zero)</div>   <br/> <a href=\"https://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression/\">Ref</a> "}, {"front": "In linear regression, what is the interpretation of a coefficient?", "back": "The mean change in the dependent variable for a unit change in an independent variable (i.e., the slope), holding all other variables constant.<div><br></div>   <br/> <a href=\"https://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression/\">Ref</a> "}, {"front": "In linear regression, how is R<sup>2</sup>&nbsp;interpreted?", "back": "Explained variance.<div><br></div><div>The proportion of variance in the dependent variable explained by the independent variables.<div><br></div><div>\\[R^{2} = 1 - \\frac{\\sum{\\text{residuals}^2}}{\\text{variance} * n} <br>= 1 - \\frac{\\sum{(y_i - \\hat{y}_i)^2}}{\\sum{(y_{i} - \\bar{y})^2}}\\]</div></div><div><br></div><div>(Also called coefficient of determination)</div>   <br/> <a href=\"https://statisticsbyjim.com/regression/interpret-r-squared-regression/\">Ref</a> "}, {"front": "What are some methods of dealing with class imbalance?", "back": "<b>Weight</b>: minority instances higher<div><br><div><b>Metrics</b>: precision, recall, F1, normalized accuracy (kappa), AUC...</div><div><br></div><div><b>Resample</b>: Upsample small classes or downsample large ones</div><div><br></div><div><b>Interpolate</b>: Generate synthetic samples by interpolation or SMOTE (randomly-weighted combinations of neighbors)</div></div><div><br></div><div><b>Bag</b>: Divide majority class into N subsets of same size as minority, train N classifiers, combine</div>   <br/> <a href=\"http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\">Ref</a> "}, {"front": "Explain the bias-variance tradeoff", "back": "<div><b>Reducing&nbsp;<font color=\"#aa3934\">Bias&nbsp;</font></b><b>Increases&nbsp;<font color=\"#009293\">Variance </font></b>(and vice-versa)<br></div><div><div><br></div></div><div>Reducing error (with more model complexity) generally results in higher sensitivity to changes in training data and worse generalization (overfitting).</div><div><img src=\"/static/uploads/data_science_interviews/biasvariance.png\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\">Ref</a> "}, {"front": "Explain bagging", "back": "<b>B</b>ootstrap <b>ag</b>gregating<div>Train multiple models on subsamples and average predictions to reduce variance.<div><br></div><div>Usually uses \"strong,\" low-bias models.<br></div></div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/500px-Ensemble_Bagging.svg.png\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Bootstrap_aggregating\">Ref</a> "}, {"front": "Explain boosting", "back": "Sequentially training models, weighting <i>mispredicted</i>&nbsp;samples&nbsp;<i>higher</i> in the next model, and combining by accuracy.<div><br></div><div>Reduces bias.</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/1000px-Ensemble_Boosting.svg.png\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Boosting_%28machine_learning%29\">Ref</a> "}, {"front": "How do decision trees work, high-level?", "back": "Recursively split the data into groups based on most discriminating feature; each leaf gives a prediction.<div><br></div><div><img src=\"/static/uploads/data_science_interviews/decision-tree2.svg\"><br></div>   <br/> <a href=\"https://victorzhou.com/blog/intro-to-random-forests/\">Ref</a> "}, {"front": "Bayes Theorem", "back": "\\[P(A|B) = \\frac{P(B|A) \\ \\ P(A)}{P(B)}\\]   <br/> <a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\">Ref</a> "}, {"front": "What is logistic regression?", "back": "Linear regression squashed to the range [0, 1] with a logistic function to do binary classification.<div><br></div><div>\\[L(\\textbf{x}) = \\frac{1}{1 + e^{-\\textbf{xw} + b}}\\]</div><div><br></div><div>\\(x\\) = vector of independent variables</div><div>\\(w\\) = learned weight vector</div><div>\\(b\\) = learned bias</div><div><br></div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/1G3imr4PVeU1SPSsZLW9ghA.png\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\">Ref</a> "}, {"front": "Logit (Log-Odds) Function", "back": "\\[logit(p) = \\ln\\frac{p}{1 - p}\\]<div>Transforms probabilites to log-odds.<br></div><div>Inverse of the logistic function.<br></div><div><br><div><img src=\"/static/uploads/data_science_interviews/350px-Logit.svg.png\"><br></div><div><br></div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Logit\">Ref</a> "}, {"front": "When can logistic regression fail?", "back": "Too many variables<div><br></div><div>Collinear samples</div><div><br></div><div>Empty classes</div><div><br></div><div>Perfectly separated samples</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Logit\">Ref</a> "}, {"front": "How can you evaluate the results of logistic regression?", "back": "Cross-validation of metrics (accuracy, precision, recall, F1, AUC...)<div>\\(\\chi^2\\) between labels and predictions<div><div><div>Likelihood ratio</div></div><div>Wald test<br></div></div><div><div><br></div><div><br></div></div><div><br></div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Logistic_regression#Evaluating_goodness_of_fit\">Ref</a> "}, {"front": "How does k-means clustering work?", "back": "Start with k random samples as centroids<div>Assign each sample to nearest centroid</div><div>Recompute centroids (average of cluster samples)</div><div>Repeat until stationary</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/558px-KMeans-Gaussian-data.svg.png\"><br></div>   <br/> <a href=\"https://towardsdatascience.com/the-anatomy-of-k-means-c22340543397\">Ref</a> "}, {"front": "How do you choose k in k-means clustering?", "back": "<div>The \"elbow method\"</div><div><br></div><div>Plot between-group variance out of total variance for each <i>k</i>, pick a point of diminishing returns.</div><div><br></div><div><br></div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/clustering-elbow-method.jpg\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set\">Ref</a> "}, {"front": "How does DBSCAN clustering work?", "back": "Finds areas of high density (many neighbors)<div><br><div>Grows clusters of core points having at least <i>m</i> neighbors within distance <i>eps</i></div><div>Plus border points within <i>eps</i></div><div>Other points are outliers (noise)</div><div><br></div><div><br></div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/dbscan.png\"><br></div></div>   <br/> <a href=\"https://scikit-learn.org/stable/modules/clustering.html#dbscan\">Ref</a> "}, {"front": "How does spectral clustering work?", "back": "Compute pairwise distance matrix<div>Reduce dimensionality (PCA)<div>Use k-means</div><div><br></div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/spectral-clustering.png\"><br></div></div><div>Dimensionality reduction involves computing the eigenvalues a.k.a. spectrum of a matrix; hence the name.</div>   <br/> <a href=\"https://towardsdatascience.com/spectral-clustering-82d3cff3d3b7\">Ref</a> "}, {"front": "How does hierarchical clustering work?", "back": "Usually bottom up (agglomerative):<div><br><div>Start with all points as clusters</div><div>Merge close clusters (distance: sum-squared, mean, max)</div><div>Repeat until target # clusters remain.</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/hierarchical-clustering.png\"><br></div></div>   <br/> <a href=\"https://www.displayr.com/what-is-hierarchical-clustering/\">Ref</a> "}, {"front": "How do you evaluate clustering quality (without ground-truth labels)?", "back": "Compare within-cluster distances to between-cluster distances.<div><br></div><div>Example: Silhouette Coefficient<br><div><br></div><div><div><img src=\"/static/uploads/data_science_interviews/cluster-evaluation.jpg\"><br></div></div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation\">Ref</a> "}, {"front": "What is A/B testing, statistically speaking?<div>How do you do it, at a very high level?</div>", "back": "Significance testing (hypothesis testing). &nbsp;<div><br><div>Try two versions of something, compare their metrics with a significance test.</div><div><br></div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/A/B_testing\">Ref</a> "}, {"front": "Significance test for binary (binomial) outcomes&nbsp;<div>(e.g., conversions out of visitors)</div>", "back": "<div>\\(\\chi^2\\) with 1 d.o.f.</div><div><br></div><div>or, for small samples or unbalanced classes, Fischer's Exact Test.</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Fisher%27s_exact_test\">Ref</a> "}, {"front": "Significance test for a continuous (normal-ish) variable<div>(e.g., spend per user)</div>", "back": "Welch's t-test<div><br></div><div><img src=\"/static/uploads/data_science_interviews/210px-Gaussian_distribution.svg.png\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/A/B_testing#Common_test_statistics\">Ref</a> "}, {"front": "Significance test for multiple count (multinomial) data<div>(e.g., number of each product purchased)</div>", "back": "<div>\\(\\chi^2\\)</div><div>with \\(k - 1\\) d.o.f. for \\(k\\) counts/classes.</div><div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/A/B_testing#Common_test_statistics\">Ref</a> "}, {"front": "What are the assumptions for (unpaired) generic t-tests?", "back": "Independent samples<div><br></div><div>Normal distribution<div><br></div><div>(Non-normal works for large N)</div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Student's_t-test\">Ref</a> "}, {"front": "One-sample t-test statistic formula", "back": "\\[t = \\frac{\\text{difference of means}}{\\text{std err of mean}} = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{N}}\\]   <br/> <a href=\"https://en.wikipedia.org/wiki/Student's_t-test#One-sample_t-test\">Ref</a> "}, {"front": "Welch's t-test statistic formula<div>(two-sample (independent), unpaired, unequal sizes, unequal variance)<br></div>", "back": "[t = \\frac{\\text{difference of means}}{\\text{std err of mean}} = \\frac{\\overline{x_1} - \\overline{x_2}}{\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_2^2}{N_2}}}\\] "}, {"front": "What is a p-value (in significance testing)?", "back": "The probability that, given no significance, you mistakenly find significance.<div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/P-value\">Ref</a> "}, {"front": "What is the danger of repeated significance testing (the multiple comparisons problem)?<div>How do you handle it?</div>", "back": "Some tests will falsely show significance by random chance.<div><br></div><div>The Bonferroni correction: just divide your significance level by the number of tests</div><div>\\[ \\alpha^* = \\frac{\\alpha}{m} \\]</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Multiple_comparisons_problem\">Ref</a> "}, {"front": "What is the problem with stopping an A/B test as soon as you see a significant result?<div>How do you avoid it?</div>", "back": "You may get falsely significant results, as you are effectively running many significance tests (the multiple comparisons problem).<div><br></div><div>Set the test sample size(s) in advance to get the power you need and don't stop early.</div>   <br/> <a href=\"http://www.evanmiller.org/how-not-to-run-an-ab-test.html\">Ref</a> "}, {"front": "What is regularization, and what problem does it address?", "back": "Adding information to a model, often constraints, to avoid overfitting and/or make a problem well-defined.<div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Regularization_(mathematics)\">Ref</a> "}, {"front": "What is L2 regularization and how does it work?", "back": "Adding a term for the L2 norm of the weights to the loss function of a model.<div><br><div>Penalizes large weights to reduce variance and overfitting.</div><div><br></div><div>Use with linear regression is called Ridge regression.</div></div>   <br/> <a href=\"https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261\">Ref</a> "}, {"front": "What is L1 regularization and how does it work?", "back": "Adding a term for the L1 norm of the weights to the loss function of a model.<div><br><div>Penalizes a large <i>number</i> of (non-zero) weights, for feature selection and simpler models.</div><div><br></div><div>Use with linear regression is called LASSO.<br></div><div><br></div><div><i>[Ideally you'd use the L0 norm - the number of non-zeros - directly.&nbsp; L1 is a convex approximation.]</i></div><div><br></div><div><br></div></div>   <br/> <a href=\"https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261\">Ref</a> "}, {"front": "How does gradient descent work?", "back": "Minimize the loss function with respect to the weights by taking small steps in the opposite direction of the gradient.<div><br></div><div>\\[\\texttt{weights -= learning rate * gradient} \\\\ w \\leftarrow w - \\eta \\nabla J(w) \\]</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/gradient-descent.png\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">Ref</a> "}, {"front": "How does stochastic gradient descent work?", "back": "Like gradient descent, but at at each step, compute the gradient and update the weights using only a small number of random samples.<div><div><br></div><div>More time and space efficient.<div><br></div><div><img src=\"/static/uploads/data_science_interviews/sgd.png\"><br></div></div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\">Ref</a> "}, {"front": "How does k-nearest neighbors work?", "back": "Training just stores samples for fast neighbor lookup (kd-tree). &nbsp;<div>Prediction finds the k nearest neighbors and combines their labels&nbsp;</div><div>(majority for classification, average for regression)<div><br></div><div><img src=\"/static/uploads/data_science_interviews/KnnClassification.svg\"><br></div><div><br></div><div><br></div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\">Ref</a> "}, {"front": "The Curse of Dimensionality says that, as dimensions increase...", "back": "<div><b>Volume of the space grows exponentially</b>, requring exponentially more data to learn functions.<br></div><div><br></div><div><b>Distance metrics lose usefulness</b>; relative distances between near and far points approach zero.</div><div><br></div><div><b>Distance to the center or mean increases</b>; \"every point is an outlier\"</div><div><br></div><div><b>Any partition of samples becomes linearly separable</b> (Cover's Theorem), leading to overfitting.</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\">Ref</a> "}, {"front": "What is the definition of log loss (a.k.a. cross-entropy), and why is it used?", "back": "\\[ -\\frac{1}{N}\\sum_{i=1}^n {y_i\\log(p_i) + (1 - y_i)\\log(1 - p_i)} \\]<div><br></div><div>Binary classification metric, measures similarity of two probability distributions, punishes extreme confidence. &nbsp;<br></div><div>Natural error function for logistic regression.</div>   <br/> <a href=\"https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\">Ref</a> "}, {"front": "Define precision", "back": "<div>\\[ \\frac{\\text{true predicted positives}}{\\text{all predicted positives}} \\]</div>\\[ \\frac{TP}{TP + FP} \\]   <br/> <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">Ref</a> "}, {"front": "Define recall (sensitivity)", "back": "<div><div>True Positive Rate</div><div>\\[ \\frac{\\text{correctly predicted positives}}{\\text{actual positives}} \\]</div><div>\\[ \\frac{TP}{TP + FN} \\]</div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">Ref</a> "}, {"front": "Define specificity", "back": "True Negative Rate<div>\\[ \\frac{TN}{TN + FP} \\]</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Sensitivity_and_specificity\">Ref</a> "}, {"front": "Define F1 measure", "back": "F1 = \\( 2 \\frac{precision * recall}{precision + recall} \\)<div><br></div><div>The harmonic mean of precision and recall</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall#F-measure\">Ref</a> "}, {"front": "Define the ROC curve", "back": "Plot of true positive rate against false positive rate.<div><br></div><div><img src=\"/static/uploads/data_science_interviews/roc.svg\"><br></div><div><br></div><div>TP = recall = sensitivity</div><div>FP = 1 - specificity</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\">Ref</a> "}, {"front": "Define AUC", "back": "Area under the ROC curve.<div><br><div>The probability that a random positive sample scores higher than a random negative sample.</div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\">Ref</a> "}, {"front": "How does a random forest work?", "back": "Bagging multiple decision trees,<div>randomly sampling both the samples and features</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/rfc_vs_dt1.png\"><br></div>   <br/> <a href=\"https://victorzhou.com/blog/intro-to-random-forests/\">Ref</a> "}, {"front": "What is statistical power?<div>What factors influence power?</div>", "back": "Probability that a test finds significance given it is actually there.<div><br></div><div>The significance level (\\(\\alpha\\)), desired effect size, and number of samples affect the power.</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Statistical_power\">Ref</a> "}, {"front": "What is SVD mathematically?", "back": "<div>Factoring a matrix \\( M \\) into \\( U\\Sigma V^T \\) where</div><div><div>\\[\\begin{align*}&nbsp; M &amp;= m \\times n \\\\ U &amp;= \\text{orthogonal } m \\times m&nbsp; \\\\&nbsp; \\Sigma &amp;= \\text{singular values (diagonal } m \\times n )&nbsp; &nbsp;\\\\&nbsp; V &amp;= \\text{principal components (orthogonal } n \\times n ) \\end{align*} \\]</div><div>Geometrically: a rotation / stretch / rotation onto the principal components.</div></div><div><img src=\"/static/uploads/data_science_interviews/849px-Singular-Value-Decomposition.svg.png\"><br></div>   <br/> <a href=\"http://gregorygundersen.com/blog/2018/12/10/svd/\">Ref</a> "}, {"front": "Law of Total Probability", "back": "<div><br></div><div><div>\\[ \\text{P}(A) = \\sum_i \\text{P}(A \\cap B_i) <br>=&nbsp;\\sum_i \\text{P}(A \\mid B_i) \\ \\text{P}(B_i) \\]</div></div><div>For a partition of the sample space \\( B_i \\)<br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Law_of_total_probability\">Ref</a> "}, {"front": "Probability Multiplication Rule (Joint Probability)", "back": "\\[ \\begin{align} <br>\\text{P}(A \\cap B) = \\text{P}(A \\mid B) \\cdot \\text{P}(B) \\\\<br>= \\text{P}(B \\mid A) \\cdot \\text{P}(A) <br>\\end{align} \\]   <br/> <a href=\"https://www.stattrek.com/probability/probability-rules.aspx\">Ref</a> "}, {"front": "Binomial Distribution", "back": "The probability of exactly \\(k\\)&nbsp;successes in \\(n\\)&nbsp;independent trials, each with probability&nbsp;\\(p\\)<div><br><div><div>\\[ {n \\choose k} p^k(1-p)^{n-k} \\\\ \\]</div><div>\\[ \\mu = np \\\\ \\]</div><div>\\[ \\sigma^2 = np(1 - p) \\]</div></div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Binomial_distribution\">Ref</a> "}, {"front": "Poisson Distribution", "back": "The probability of exactly \\(k\\) independent events in an interval, given an average of \\( \\lambda \\) events per interval<div><br></div><div><div>\\[ \\frac{\\lambda^k e^{-\\lambda}}{k!} \\\\ \\]</div><div>\\[ \\mu = \\lambda \\\\ \\]</div><div>\\[ \\sigma^2 = \\lambda \\]</div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Poisson_distribution\">Ref</a> "}, {"front": "Geometric Distribution", "back": "Probability of \\(k\\) independent trials to reach one success, each with probability \\(p\\)<div><br></div><div>\\[ (1 - p)^{k - 1}p \\\\ \\]</div><div>\\[ \\mu = \\frac{1}{p} \\\\ \\]</div><div>\\[ \\sigma^2 = \\frac{1 - p}{p^2} \\]</div><div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Geometric_distribution\">Ref</a> "}, {"front": "Negative Binomial Distribution", "back": "Probability of \\(k\\) successes before \\(r\\) failures, each independent with probability \\(p\\)<div><br></div><div>\\[ {k + r - 1 \\choose k} p^k(1 - p)^r \\\\ \\]</div><div>\\[ \\mu = \\frac{pr}{1-p} \\\\ \\]</div><div>\\[ \\sigma^2 = \\frac{pr}{(1-p)^2} \\]</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Negative_binomial_distribution\">Ref</a> "}, {"front": "Hypergeometric Distribution", "back": "Probability of \\(k\\) successes in&nbsp;\\(n\\) draws without replacement from&nbsp;\\(N\\) items containing&nbsp;\\(K\\) successes<div><br></div><div>\\[ \\frac{\\binom{K}{k} \\binom{N - K}{n - k}}{\\binom{N}{n}} \\]<br></div><div>\\[ \\mu = n\\frac{K}{N} \\\\ \\]</div><div>\\[ \\sigma^2 = n{K\\over N}{(N-K)\\over N}{N-n\\over N-1} \\]</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Hypergeometric_distribution\">Ref</a> "}, {"front": "Normal Distribution", "back": "\\[ \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left ( \\frac{x - \\mu}{\\sigma} \\right )^2} \\]   <br/> <a href=\"https://en.wikipedia.org/wiki/Normal_distribution\">Ref</a> "}, {"front": "Define linear independence for a set of vectors", "back": "No vector can be written as a linear combination of the others.<div><br></div><div>That is, \\( \\sum_i {a_i {\\vec{\\textbf{v}}}_i} = \\vec{\\textbf{0}} \\) has only the solution that all \\(a_i = 0\\) .</div><div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/linear-independence.png\"><br></div><div><br></div><div>Can be determined by gaussian elimination on the above equation.</div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Linear_independence\">Ref</a> "}, {"front": "Define orthogonality for vectors", "back": "The dot product is zero.<div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Orthogonality\">Ref</a> "}, {"front": "Define the span of a set of vectors", "back": "The set of all linear combinations of the vectors.<div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Linear_span\">Ref</a> "}, {"front": "Define the rank of a matrix", "back": "The dimension of the space spanned by the columns<div>or&nbsp;<div>The number of linearly independent vectors among the columns</div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Rank_(linear_algebra)\">Ref</a> "}, {"front": "Define the eigenvectors and eigenvalues of a matrix", "back": "<div>The vectors for which matrix multiplication is the same as scalar multiplication.</div><div><br></div><div>The eigenvectors \\(\\textbf{x}\\) and corresponding (scalar) eigenvalues \\(\\lambda\\) for a matrix \\(A\\) are those for which</div><div>\\[ A \\textbf{x} = \\lambda \\textbf{x} \\]</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/Eigenvector-1.png\"><br></div>   <br/> <a href=\"https://wiki.pathmind.com/eigenvector\">Ref</a> "}, {"front": "Define a basis of a vector space", "back": "A set of linearly independent vectors that span the space<div>(\\(n\\) lin. indep. vectors for an \\(n\\)-dimensional space)</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Basis_(linear_algebra)\">Ref</a> "}, {"front": "State as many equivalencies of the Invertible Matrix Theorem as you can.<div>An \\(n \\times n\\) matrix \\(A\\) is invertible if and only if...<br></div>", "back": "<div>The equation \\(Ax = 0\\) has only the trivial solution \\(x = 0\\).<br></div><div><div>The determinant of \\(A\\) is nonzero.</div><div>The rows and columns of \\(A\\) are linearly independent.</div><div>The rows and columns of \\(A\\) span and form a basis for \\(\\mathbb{R}^n\\).<br></div></div><div>\\(x \\to Ax\\) is one-to-one and onto.</div><div>The transpose is invertible.</div><div>The inverse of \\(A\\) exists and is unique.</div><div>\\(A\\) has rank \\(n\\).</div><div>The null space of \\(A\\) is \\(0\\).</div><div>\\(A\\) has \\(n\\) nonzero eigenvalues and singular values.</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem\">Ref</a> "}, {"front": "How do support vector machines work?", "back": "SVMs find the hyperplane of maximum distance (margin) from the nearest samples of each class (the \"support vectors\").<div><br></div><div>For non-separable data, we minimize the distance from the margin (for misclassified samples) plus the inverse of the margin size.</div><div><br></div><div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Support_vector_machine\">Ref</a> "}, {"front": "What is principal component analysis?", "back": "<div>PCA finds a \"rotation\" (coordinate system, basis) of data to linearly independent features ordered by highest possible variance.</div><div><br></div><div>Used for dimensionality reduction (by taking top few) or&nbsp;decorrelating features.</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/PCA.jpg\"><br></div>   <br/> <a href=\"https://stats.stackexchange.com/a/140579\">Ref</a> "}, {"front": "How is PCA related to SVD?", "back": "<div>Given the SVD of \\(X = U\\Sigma V^T\\):</div><div><br></div><div>\\(V\\) : principal components (the right-singular vectors)</div><div>&nbsp;</div><div>\\(\\Sigma\\) : component weights square roots (the singular values).</div><div><br></div><div>\\(V\\) and \\(\\sqrt{\\Sigma}\\) are the eigenvectors/values of the covariance matrix \\(X^TX = V\\Sigma^2V^{-1}\\).</div>   <br/> <a href=\"http://gregorygundersen.com/blog/2018/12/10/svd/\">Ref</a> "}, {"front": "How do you decide the number of components to keep in PCA?", "back": "The \"elbow method\"&nbsp;<div>Plot explained variance vs. number of components and pick a point of diminishing returns.<div><br></div><div>The explained variance is the sum of the retained eigenvalues divided by the sum of all eigenvalues.</div></div>   <br/> <a href=\"https://blogs.sas.com/content/iml/2017/08/02/retain-principal-components.html\">Ref</a> "}, {"front": "Define covariance", "back": "\\[ \\begin{align} <br>\\operatorname{cov}(X,Y) &amp;= \\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]} \\\\ <br>&amp;= \\operatorname{E}\\left[X Y\\right] - \\operatorname{E}\\left[X\\right] \\operatorname{E}\\left[Y\\right] <br>\\end{align} \\]<br><div><br></div><div>For discrete data \\(x_i\\) and \\(y_i\\):</div><div>\\[ \\frac{1}{N} \\sum_i{(x_i - \\bar{x})(y_i - \\bar{y})} \\]</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Covariance\">Ref</a> "}, {"front": "What are some techniques for feature selection?", "back": "<div>Algorithms with built-in selection</div><div><br></div><div>Univariate stats / ranking<br></div><div><br></div><div>Recursive feature elimination</div><div><br></div><div>Dimensionality reduction<br></div><div><br></div><div><div>Forward or backward selection</div></div>   <br/> <a href=\"https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\">Ref</a> "}, {"front": "What is bootstrapping?", "back": "Computing something many times on random subsamples of the data (with replacement)&nbsp;<div><br><div>To estimate uncertainty or reduce variance</div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\">Ref</a> "}, {"front": "What are the limitations of PCA?", "back": "Sensitive to the scale of features<div>Assumes features with less variance are less important</div><div>Assumes (Gaussian) variance accurately characterizes features</div><div>Assumes features are orthogonal</div><div>Only performs linear transformations (but see kernel PCA)</div><div>Only removes linear correlation</div>   <br/> <a href=\"https://arxiv.org/pdf/1404.1100.pdf\">Ref</a> "}, {"front": "Define cosine similarity", "back": "<div>The cosine of the angle between two vectors.</div>\\[ \\cos(\\theta) = \\frac{a \\cdot b}{||a|| ||b||} \\]<div>1 for parallel vectors</div><div>0 for orthogonal vectors</div><div>-1 for opposite direction<br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\">Ref</a> "}, {"front": "How does Naive Bayes classification work?", "back": "<div>Compute the probability of each class given a sample, using Bayes' Theorem and assumping independence</div><div>Return the most likely class</div><div><br></div><div>For class \\(C_k\\) and sample \\(x = (x_1, ..., x_n)\\),&nbsp;<br></div><div><br></div><div>\\[ \\begin{align} <br>P(C_k \\mid x) &amp;= \\frac{P(x \\mid C_k)\\ P(C_k)}{P(x)} &amp;&amp; \\text{Bayes Theorem} \\\\ <br>&amp;= \\frac{P(C_k)\\ P(x_1 \\mid C_k) \\cdots P(x_n \\mid C_k)}{P(x_1) \\cdots P(x_n)} &amp;&amp; \\text{Assuming independent } x_i <br>\\end{align} \\]</div><div><br></div><div>Continuous: assume normal distribution&nbsp;</div><div>Categorical: tabulate probabilities directly</div><div>Counts: multinomial distribution<br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\">Ref</a> "}, {"front": "Describe power-law distributions. &nbsp;<div>Why are they difficult to work with?</div>", "back": "\\[ f(x) = cx^{-\\alpha} \\]<div><br></div><div><img src=\"/static/uploads/data_science_interviews/power-law.png\"><br></div><div><br></div><div>The mean and variance are infinite for values of \\(\\alpha \\le 2\\) or \\(3\\) (resp.), which are common, invalidating typical assumptions of normality.</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Power_law\">Ref</a> "}, {"front": "How do you determine the sample size needed to measure a given difference with a certain confidence?", "back": "<div>View it as a \\(t\\)-test of significance of the difference \\(\\delta\\):</div><div>\\[ \\frac{\\delta}{\\sigma \\over \\sqrt{n}} \\gt t \\]<br></div><div><div>Then&nbsp;</div><div>\\[ n \\gt \\frac{t^2 \\sigma^2}{\\delta^2} \\]</div></div><div>where \\(t\\) is the critical value for the desired level of confidence (with high d.o.f.), or equivalently, the corresponding standard normal deviate, e.g. \\(95\\% \\to t = 1.96\\). &nbsp;</div><div><br></div>   <br/> <a href=\"http://www.itl.nist.gov/div898/handbook/ppc/section3/ppc333.htm\">Ref</a> "}, {"front": "What is a quick formula for confidence intervals of normal-ish data?", "back": "\\[ \\mu \\pm z\\sigma \\]<div>or in general<br></div><div>\\[ \\text{ statistic } \\pm \\text{ critical value } \\times \\text{ std dev of the statistic } \\]<br></div><div><div><br></div><div>\\(z\\) is the critical value of the test statistic corresponding to the desired confidence (e.g., \\(z = 1.96\\) for \\(95\\%\\) confidence with normal data), and \\(\\sigma\\) is the stddev of the statistic (e.g., the SEM = \\(\\sigma \\over \\sqrt{n}\\) for a mean), <i>not the population</i>.</div></div>   <br/> <a href=\"http://stattrek.com/estimation/confidence-interval.aspx\">Ref</a> "}, {"front": "Define conditional probability", "back": "\\[ P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} \\]   <br/> <a href=\"https://en.wikipedia.org/wiki/Conditional_probability\">Ref</a> "}, {"front": "Define the silhouette score", "back": "<div>\\[ \\frac{\\text{nearest_dist - within_dist}}{\\max(\\text{nearest_dist, within_dist})} \\]<br></div><div>Using average distance from one sample to all points <i>within</i> in its own cluster, or the <i>nearest</i> other cluster.</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Silhouette_(clustering)\">Ref</a> "}, {"front": "Why is the Central Limit Theorem important?", "back": "It lets you approximate most any random variable with a normal variable, and quantifies the error of the approximation.<div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Central_limit_theorem\">Ref</a> "}, {"front": "In what sense is dimensionality reduction by PCA optimal?", "back": "The basis formed by the \\(k\\) largest principal components minimizes the least-squares projection error in \\(k\\) dimensions.<br>   <br/> <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Ref</a> "}, {"front": "What does PCA compute (in matrix-theoretic terms)?", "back": "The eigenvectors and values of the covariance matrix \\(X^TX\\)<div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Ref</a> "}, {"front": "What is the \\(\\chi^2\\) test, and how is it computed?", "back": "<div>Compares an actual discrete distribution \\(a_i\\) to an expected distribution \\(x_i\\).</div><div>\\[ \\chi^2 = \\sum^k_{i=1}{\\frac{(x_i-a_i)^2}{x_i}} \\]<br></div><div>Compare to critical value from table with \\(k-1\\) d.o.f.<br></div><div><br></div><div>The errors (differences) must be independent and normally distributed.</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/chi-square.png\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Chi-squared_test\">Ref</a> "}, {"front": "Define correlation", "back": "Covariance normalized by the standard deviations<div>\\[<br>Corr(X,Y) = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y} = <br>\\frac{\\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]}}{\\sigma_X \\sigma_Y}<br>\\]<br></div><div><br></div><div>For discrete data \\(x_i\\) and \\(y_i\\):</div><div>\\[ \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})} {\\sqrt{\\sum (x_i-\\bar{x})^2 \\sum (y_i-\\bar{y})^2 }} \\]</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Correlation_and_dependence\">Ref</a> "}, {"front": "What are the advantages of decision trees?", "back": "Interpretable<div>Minimal feature engineering</div><div>Nonlinear model</div><div>Provides feature importance</div><div>Prediction is efficient</div>   <br/> <a href=\"https://scikit-learn.org/stable/modules/tree.html\">Ref</a> "}, {"front": "What are the disadvantages of decision trees?", "back": "Prone to overfitting (high variance)<div>Hard to learn some simple functions (parity, xor)</div><div>Decision boundaries always parallel to axes</div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning#Limitations\">Ref</a> "}, {"front": "What are the advantages of SVMs?", "back": "<div>Can model non-linear relationships with kernels</div><div>Finds \"best\" model&nbsp;for given hyperparams, since error function has global minimum</div>   <br/> <a href=\"https://stats.stackexchange.com/questions/24437/advantages-and-disadvantages-of-svm\">Ref</a> "}, {"front": "What are the disadvantages of k-nearest neighbors?", "back": "<div>Sensitive to feature scaling</div><div>Suffers from class imbalance</div><div>Distance metrics lose usefulness in high dimensions (curse of dimensionality)<br></div><div>Model size grows linearly with data (must store all samples)</div>   <br/> <a href=\"https://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/\">Ref</a> "}, {"front": "What are the advantages of Naive Bayes classification?", "back": "Interpretable<div>Efficient</div><div>Compact models</div><div>Works with small data<br></div>   <br/> <a href=\"https://easyai.tech/en/ai-definition/naive-bayes-classifier/\">Ref</a> "}, {"front": "What are the disadvantages of Naive Bayes classification?", "back": "Assumes features are independent (given the class)<div><div>Assumes a distribution for continuous features (usually normal)</div><div>Does not handle sparse data well<br><div>Fixed-sized model; diminishing returns with more data</div></div></div>   <br/> <a href=\"https://www.quora.com/What-are-the-disadvantages-of-using-a-naive-bayes-for-classification\">Ref</a> "}, {"front": "What are the disadvantages of SVMs?", "back": "Slow and large in both training and prediction; don't scale well<div><div>Not great with multiclass problems</div><div><br></div></div>   <br/> <a href=\"https://stats.stackexchange.com/questions/24437/advantages-and-disadvantages-of-svm\">Ref</a> "}, {"front": "Describe a project or research you've done", "back": "lt;your response here&gt; "}, {"front": "Describe a challenge you faced and how you overcame it", "back": "lt;your response here&gt; "}, {"front": "Describe a mistake or failure in your previous experience, and how you overcame it", "back": "lt;your response here&gt; "}, {"front": "Describe something you've enjoyed in your previous experience", "back": "lt;your response here&gt; "}, {"front": "Describe a conflict in your previous experience, and how you handled it", "back": "lt;your response here&gt; "}, {"front": "Describe something from your previous experience that you would do differently", "back": "lt;your response here&gt; "}, {"front": "Describe a time when you took initiative", "back": "lt;your response here&gt; "}, {"front": "Describe a time when you worked with others to solve a problem", "back": "lt;your response here&gt; "}, {"front": "Describe a change in project scope or schedule in your previous experience, and how you handled it", "back": "lt;your response here&gt; "}, {"front": "How does an artificial neuron (perceptron) work?", "back": "Applies an activation function to the weighted sum of its inputs.<div><br></div><div>\\[ y = f \\left( \\textstyle \\sum w_i x_i \\right) \\]</div><div><br></div><div>Common activation functions are linear, step, sigmoid, tangent, rectified linear...</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/neuron.jpg\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Perceptron\">Ref</a> "}, {"front": "How does a (fully-connected, feedforward) neural network work?", "back": "Successive layers of neurons each recieve the same inputs, apply weights and an activation function, and produce an output. &nbsp;The outputs of one layer are the inputs of the next.<div><br></div><div>Neural networks are trained via backpropagation, which iteratively updates the weights in the direction that minimizes error at the outputs (often via gradient descent).<br><div><br></div><div><img src=\"/static/uploads/data_science_interviews/neural-network.gif\"><br></div><div><br></div><div><br></div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\">Ref</a> "}, {"front": "How does backpropagation work?", "back": "Backpropagation iteratively updates the weights of a neural network to minimize the error between the actual and desired outputs.<div><br></div><div>Each weight gets updated in the opposite direction of the derivative of the loss function with respect to that weight (the gradient). &nbsp;The derivatives at each layer depend on the derivatives of all successive layers (between it and the output), so the weight updates are calculated backward from the output: backpropagation.</div><div><br></div><div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Backpropagation\">Ref</a> "}, {"front": "How does the kernel trick work?", "back": "<div>Computing the results of vector dot products after non-linear mappings into high-dimesional space using only dot products in a low-dimensonal input space.</div><div><img src=\"/static/uploads/data_science_interviews/kernel-trick.png\"><br></div><div>If \\(\\phi: \\mathbb{R}^n \\to \\mathbb{R}^m\\) maps vectors to a higher-dimensional space (\\(m \\gt n)\\), then the corresponding kernel function&nbsp;</div><div>\\[ K(x,y) = \\langle \\phi(x), \\phi(y) \\rangle \\]</div><div><div>\\[ K: \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R} \\]</div></div><div><i>acts</i>&nbsp;as a dot product in higher-dimensional mapped space, but can be <i>computed</i>&nbsp;using only low-dimensional dot products \\(\\langle x,y \\rangle\\).</div>   <br/> <a href=\"http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html\">Ref</a> "}, {"front": "What is bias?", "back": "<div>Error - wrong answers, low accuracy, underfitting</div><div><br></div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/paste-e2c5000f8343de5954dd5ff4cba763763a728213.jpg\"><br></div>   <br/> <a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\">Ref</a> "}, {"front": "What is variance (in a model)?", "back": "Change in predictions with changes in training.<div><br><div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/paste-e2c5000f8343de5954dd5ff4cba763763a728213.jpg\"><br></div></div><div><br></div><div><br></div><div><i>[Technically: the out-of-sample prediction error averaged over (models trained on) many training sets.]</i></div></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\">Ref</a> "}, {"front": "What is underfitting?", "back": "Low accuracy, due to insufficient model complexity or training<div><br></div><div><img src=\"/static/uploads/data_science_interviews/paste-9468185cbf252cd0d1ec910aa6a022baaaecd658.jpg\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Overfitting\">Ref</a> "}, {"front": "What is overfitting?", "back": "Poor generalization, from too much model complexity or over training<div><br></div><div><img src=\"/static/uploads/data_science_interviews/paste-9468185cbf252cd0d1ec910aa6a022baaaecd658.jpg\"><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Overfitting\">Ref</a> "}, {"front": "What is gradient boosting?", "back": "<div>Boosting where sucsessive models predict the&nbsp;<i>error</i>&nbsp;of the previous model; the error is equivalent to the gradient of the loss function.</div><div><br></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Gradient_boosting\">Ref</a> "}, {"front": "How are decision trees constructed?", "back": "<div><div>Out of all values of all features, choose the one that best splits the targets<br></div></div><div>\"Best\" means groups similar targets together</div><div>(Gini impurity or information gain for classification, variance for regression)</div><div>Create a child node for the split value</div><div>Split the data and recurse</div><div>Stop at given depth, leaf size, purity, etc.</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/decision-tree2.svg\"><br></div>   <br/> <a href=\"https://victorzhou.com/blog/intro-to-random-forests/\">Ref</a> "}, {"front": "What is the kernel trick used for?", "back": "Efficiently mapping data into higher-dimensional spaces where it is more easily separable.<div><br></div><div><img src=\"/static/uploads/data_science_interviews/kernel-trick.png\"><br></div>   <br/> <a href=\"http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html\">Ref</a> "}, {"front": "What is univariate feature selection?", "back": "Choose the best features as ranked by variance, correlation with labels, mutual information, etc.<div><br></div>   <br/> <a href=\"https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\">Ref</a> "}, {"front": "What is forward feature selection?", "back": "Find the best single feature via cross-validation. &nbsp;<div>Then add each remaining feature, one at a time, to find the best pair of features. &nbsp;</div><div>Repeat, adding one feature each iteration, until performance does not improve significantly.<div><br></div></div>   <br/> <a href=\"https://towardsdatascience.com/intro-to-feature-selection-methods-for-data-science-4cae2178a00a\">Ref</a> "}, {"front": "What is recursive feature elimination?", "back": "<div>Use a model that gives feature importances<br></div><div>Train with all features</div><div>Remove the least important feature(s)</div><div>Repeat until performance decreases significantly</div>   <br/> <a href=\"https://machinelearningmastery.com/rfe-feature-selection-in-python/\">Ref</a> "}, {"front": "Examples of supervised methods with built-in feature selection", "back": "Decision trees<br><div>LASSO (linear regression with L1 regularization)</div>   <br/> <a href=\"https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\">Ref</a> "}, {"front": "What is SVD intuitively?", "back": "A matrix factorization such that the most significant components form the best approximation for a given number of dimensions.<div><br></div><div>SVD is used for dimensionality reduction or efficiently approximating a matrix (as in collaborative filtering).<br><div><br></div></div>   <br/> <a href=\"http://gregorygundersen.com/blog/2018/12/10/svd/\">Ref</a> "}, {"front": "How do non-linear SVMs work?", "back": "<div>SVMs can find non-linear separating hyperplanes by mapping data into a higher number of dimensions with the kernel trick.</div><div><br></div><div>This works because SVMs can be built using only dot products on the data.</div><div><br></div><div><img src=\"/static/uploads/data_science_interviews/svm.png\"></div>   <br/> <a href=\"https://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_classification\">Ref</a> "}]